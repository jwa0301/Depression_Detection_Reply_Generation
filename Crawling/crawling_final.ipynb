{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9831a3",
   "metadata": {},
   "source": [
    "# 에브리타임 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce83341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Options\n",
    "webdriver_options = Options()\n",
    "\n",
    "# Specify the user-agent\n",
    "user_agent = \"Mozilla/5.0 (Linux; Android 9; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.83 Mobile Safari/537.36\"\n",
    "\n",
    "# Add the user-agent to the options\n",
    "webdriver_options.add_argument('user-agent=' + user_agent)\n",
    "\n",
    "# Create a Chrome driver with the specified options\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=webdriver_options)\n",
    "# Set implicit wait time\n",
    "driver.implicitly_wait(1)\n",
    "\n",
    "# 로그인 페이지에서 로그인하기\n",
    "url = 'https://everytime.kr/login'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#우울증게시판 크롤링\n",
    "base_url = 'https://everytime.kr/413131'\n",
    "driver.get(base_url)\n",
    "\n",
    "title = []\n",
    "contents = []\n",
    "\n",
    "for page in tqdm(range(1,11)):\n",
    "    url = base_url + f'/p/{page}'\n",
    "    rand_value = random.uniform(5, 8)\n",
    "    time.sleep(rand_value)\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    #제목과 내용 태그 모두 찾음(한 페이지당 20개)\n",
    "    h2_tags = soup.find_all('h2')\n",
    "    time.sleep(rand_value)\n",
    "    p_tags = soup.find_all('p', attrs={'class':'medium'})\n",
    "    \n",
    "    #글 제목\n",
    "    title_text = [h2_tags[i].text for i in range(20)]\n",
    "    title.extend(title_text)\n",
    "    \n",
    "    #글 내용\n",
    "    contents_text = [p_tags[i].text for i in range(20)]\n",
    "    contents.extend(contents_text)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame({'title' : title,\n",
    "                  'content' : contents})\n",
    "df.to_csv('everytime_depression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0878bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Length 히스토그램 그리기\n",
    "plt.hist(df['combined'].apply(len), bins=20, color='blue', edgecolor='black')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Text Length Histogram for Combined Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5282964",
   "metadata": {},
   "source": [
    "# 네이버 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78509a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import csv\n",
    "import requests\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f082774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본인의 네이버 API 키 발급 필요\n",
    "client_id = \"\"\n",
    "client_secret = \"\" \n",
    "encText = urllib.parse.quote(\"우울증\") #검색 키워드\n",
    "links = []\n",
    "\n",
    "\n",
    "for i in range(11):\n",
    "    try:\n",
    "        url = \"https://openapi.naver.com/v1/search/kin?query=\" + encText +\"&display=100\" + f\"&start={1+100*i}\" #json 결과\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "        if(rescode==200):\n",
    "            response_body = response.read()\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "\n",
    "        split = response_body.decode('utf-8').split('{')\n",
    "        for j in range(2, len(split)):\n",
    "            links.append(split[j].split('\\n')[2][9:-2])\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "df = pd.DataFrame(links)\n",
    "df.columns = ['url']\n",
    "df['url'] = df['url'].apply(lambda x: x[2:])\n",
    "df['url'] = df['url'].apply(lambda x: x.replace('\\/','/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 리스트\n",
    "titles = []\n",
    "questions = []\n",
    "answers = []\n",
    "qa_urls = []\n",
    "failed_urls = []\n",
    "a_number = []\n",
    "for i, url in enumerate(df.url):\n",
    "    print(\"---------------------------\",i,\"---------------------------\")\n",
    "    response = requests.get(url)\n",
    "    print(url)\n",
    "    \n",
    "    # candidate answer lists\n",
    "    candid_answers = []\n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        # title and question\n",
    "        try:\n",
    "            title = soup.select_one(\"#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__title > div.c-heading__title-inner > div.title\").get_text().strip()\n",
    "            question = soup.select_one(\"#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__content\").get_text().strip()\n",
    "        except:\n",
    "            try:\n",
    "                title = soup.select_one(\"#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--multiple-old > div.c-heading__title > div.c-heading__title-inner > div.title\").get_text().strip()\n",
    "                question = soup.select_one(\"#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--multiple-old > div.c-heading__content\").get_text().strip()\n",
    "            except:\n",
    "                try:\n",
    "                    title = soup.select_one(\"#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default > div.c-heading__title > div.c-heading__title-inner > div.title\").get_text().strip()\n",
    "                    question = '제목과 내용 동일'\n",
    "                except:\n",
    "                    try:\n",
    "                        title = soup.select_one(\"#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--multiple > div.c-heading__title > div.c-heading__title-inner > div.title\").get_text().strip()\n",
    "                        question = '제목과 내용 동일'\n",
    "                    except:\n",
    "                        failed_urls.append(df['url'][i])\n",
    "                        continue\n",
    "        for j in range(1,10):\n",
    "            try:\n",
    "                try:\n",
    "                    temp = soup.select_one('#answer_' + str(j)).find('div', {'class': 'se-module se-module-text'}).text\n",
    "                except:\n",
    "                    temp = soup.select_one('#answer_' + str(j)).find('div', {'class': '_endContentsText c-heading-answer__content-user'}).text\n",
    "                \n",
    "                if temp != '':\n",
    "                    titles.append(title)\n",
    "                    questions.append(question)\n",
    "                    answers.append(temp)\n",
    "                    qa_urls.append(url)\n",
    "                \n",
    "            except:\n",
    "                break\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "# Check saved data\n",
    "print(len(titles))\n",
    "print(len(questions))\n",
    "print(len(answers))\n",
    "print(len(qa_urls))\n",
    "print(len(failed_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert List to DataFrame and Save\n",
    "kin = pd.DataFrame(\n",
    "                {'title' : titles,\n",
    "                 'question' : questions,\n",
    "                 'answer' : answers,\n",
    "                 'url' : qa_urls\n",
    "                })\n",
    "\n",
    "kin.to_csv('naver_kin.csv', index=False)\n",
    "kin.to_excel('naver_kin.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
